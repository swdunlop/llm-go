Refactor Llama and LLM
[x] Add a configuration interface for language models.
    Language models come with a lot of tunables.  To abstract them with a unified interface, we need an abstract 
    configuration interface that can be combine the usual suspects, like YAML or JSON files and environment
    variables or command line flags.
[x] Add a generic interface for language model predictions as the base "llm" package.
    We want to abstract over llama.cpp in a way that lets us use other language models, like RWKV, and API services
    like llama-server.  This makes it easier for basic applications to use language models without bogging down into
    supporting each one individually.  (Although, our focus is on llama.cpp itself.)
[x] Make llama an implementation of the generic interface.
    This is the flag bearer for local LLM implementations, since it is the one we already support.
[x] Dump llama.Interface for llama.Model
[x] Split llama.Options into llama.ModelOptions and llama.PredictOptions
[x] Document all the exported types and functions.

Support Language Models Over NATS
[x] Add a NATS client implementation of the llm.Interface that lets a frontend call a backend.
    This would be a good example of how to use llama models in a distributed system, balancing the work across
    multiple backends.
[x] Ensure the NATS worker uses log/slog for logging -- this requires Go 1.21, which is unfortunate since it was only
    released on 2023-08-08.  (What the heck took so long for this to reach stable?)
[x] Add a NATS worker implentation compatible with the NATS client usable with LLM interface implementations.
[x] Add an option for client names to the NATS client.
[x] Ensure that when the worker is cancelled, it signals error 8.
[x] Ensure that when the client is cancelled, it sends an interrupt.

LLaMA Usability Improvements
[x] Add "instructions" and make "content" an array of strings, not a single string.
    This greatly simplifies telling LLaMA that there is content that cannot be edited down for generation (the
    instructions) and tells it how to edit down the content for generation (the content).
[ ] Document ModelOptions and PrefixOptions understood by llama 
    The configuration model makes it harder to know what options are available.  Adding some markdown documentation to
    the package is needed.


Bug Fixes for LLaMA
[ ] If len(tokens) == 0, llama.Model.next will panic.  (Inherited from ollama)
[ ] Why does llama.Model.Predict call the callback once with an empty output at the end of generation?

Support Language Models Over HTTP
[ ] Implement "llm server", an HTTP service that can serve pluggable LLM models.
[ ] Implement "llama-server" mimic API in "llm server"
[ ] Implement "Kobold AI" mimic API in "llm server"
[ ] Implement "OpenAI" mimic API in "llm server"
[ ] Add Autocert support.
    Nobody wants their predictions running around the internet nekkid.
[ ] Add support for API keys.
    Give the server a list of API key hashes and add a "llm apikey" command to generate them.
[ ] Add NGrok support.
    Exposing a language model over NGrok will simplify using third party AI frontends, but we need to have API keys
    first.

Catch up on GGUF Changes
[ ] Catch up to llama.cpp after their beta changes by only supporting loading GGUF for Llama. (We are currently on 
    dadbed99e65252d79f81101a392d0d6497b86caa)
[ ] Add support for GGUF to Llama

Support Better Output Constraints
[ ] Support for terminating predictions from Go.
    The prediction type should have a method that can be used to terminate prediction.  Ideally, we also want to
    rewind the prediction, since it is not always clear what is being predicted from a single token -- this is
    a fundamental problem for "token healing".
[ ] Expose the EOL and EOS tokens from the llama Interface
    When writing a prediction handler which can terminate a prediction, knowing when an "EOL" is predicted can be
    a useful signal to stop.  Simply adding EOL as a stop token does not work well.
[ ] GBNF grammars provide a different way to constrain predicted output to a required format.
[ ] Implement support for writing output constraints in Go.

Explore Adding RWKV
[ ] Add RWKV as an LLM implementation alternate to Llama.  This uses GGML and is really pretty similar to llama.cpp,
    but having a second implemenation might keep the llm package from being the llama package in a fancy coat.

Technical Documentation
[ ] Document NATS setup.
    A bootstrap task for llm might make life easier, setting up a TLS CA and generating certificates for the NATS
    workers and clients.  This might be even nicer if we embed the NATS server.
[ ] Document log/slog usage for clients to debug.
[ ] Tutorial on NATS setup.
