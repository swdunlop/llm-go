Refactor Llama and LLM
[ ] Add a configuration interface for language models.
    Language models come with a lot of tunables.  To abstract them with a unified interface, we need an abstract 
    configuration interface that can be combine the usual suspects, like YAML or JSON files and environment
    variables or command line flags.
[ ] Add a generic interface for language model predictions as the base "llm" package.
    We want to abstract over llama.cpp in a way that lets us use other language models, like RWKV, and API services
    like llama-server.  This makes it easier for basic applications to use language models without bogging down into
    supporting each one individually.  (Although, our focus is on llama.cpp itself.)
[ ] Make llama an implementation of the generic interface.
    This is the flag bearer for local LLM implementations, since it is the one we already support.
[ ] Document llama settings in llama/README.md 
[ ] Link llama/README.md from main README.md

Catch up on GGUF Changes
[ ] Catch up to llama.cpp after their beta changes by only supporting loading GGUF for Llama. (We are currently on 
    dadbed99e65252d79f81101a392d0d6497b86caa)
[ ] Add support for GGUF to Llama

Support Language Models Over NATS
[ ] Add a NATS client implementation of the llm.Interface that lets a frontend call a backend.
    This would be a good example of how to use llama models in a distributed system, balancing the work across
    multiple backends.  We probably want to support both Jetstream and normal NATS requests to get both (At Least One)
    ATLO and ATMO (At Most One), since they serve different use cases and performance guarantees.
[ ] Add a NATS server implentation compatible with the NATS client usable with LLM interface implementations.
    This is specifically targeted at llama.cpp and other local LLMs, but this could also be used to separate API keys
    from API consumers so we can just use NATS for authorization.
[ ] Add a clean NATS example server, like io-worker, that shows how to run llama models on multiple hosts as backends
    for a API / UI server.  Mimicking one of the other llama servers used by open source UIs and would make for a
    better demo.

Support Better Output Constraints
[ ] Support for terminating predictions from Go.
    The prediction type should have a method that can be used to terminate prediction.  Ideally, we also want to
    rewind the prediction, since it is not always clear what is being predicted from a single token -- this is
    a fundamental problem for "token healing".
[ ] Expose the EOL and EOS tokens from the llama Interface
    When writing a prediction handler which can terminate a prediction, knowing when an "EOL" is predicted can be
    a useful signal to stop.  Simply adding EOL as a stop token does not work well.
[ ] GBNF grammars provide a different way to constrain predicted output to a required format.
[ ] Implement support for writing output constraints in Go.

Explore Adding RWKV
[ ] Add RWKV as an LLM implementation alternate to Llama.  This uses GGML and is really pretty similar to llama.cpp,
    but having a second implemenation might keep the llm package from being the llama package in a fancy coat.

