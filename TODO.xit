Refactor Llama and LLM
[x] Add a configuration interface for language models.
    Language models come with a lot of tunables.  To abstract them with a unified interface, we need an abstract 
    configuration interface that can be combine the usual suspects, like YAML or JSON files and environment
    variables or command line flags.
[x] Add a generic interface for language model predictions as the base "llm" package.
    We want to abstract over llama.cpp in a way that lets us use other language models, like RWKV, and API services
    like llama-server.  This makes it easier for basic applications to use language models without bogging down into
    supporting each one individually.  (Although, our focus is on llama.cpp itself.)
[x] Make llama an implementation of the generic interface.
    This is the flag bearer for local LLM implementations, since it is the one we already support.
[x] Dump llama.Interface for llama.Model
[x] Split llama.Options into llama.ModelOptions and llama.PredictOptions
[x] Document all the exported types and functions.

Support Language Models Over NATS
[x] Add a NATS client implementation of the llm.Interface that lets a frontend call a backend.
    This would be a good example of how to use llama models in a distributed system, balancing the work across
    multiple backends.
[x] Ensure the NATS worker uses log/slog for logging -- this requires Go 1.21, which is unfortunate since it was only
    released on 2023-08-08.  (What the heck took so long for this to reach stable?)
[x] Add a NATS worker implentation compatible with the NATS client usable with LLM interface implementations.
[x] Add an option for client names to the NATS client.
[x] Ensure that when the worker is cancelled, it signals error 8.
[x] Ensure that when the client is cancelled, it sends an interrupt.
[x] Add NATS parameters for NKey authorization and TLS certificate authority.
[x] Support specifying the model from a directory of models and loading on demand.
    This lets the client control which model is used.
[x] Support requesting a list of models.
[ ] When n_past exceeds n_ctx, compact the context and continue.
[ ] Review use of buffered channels for stream subscriptions.
    This should not be necessary, but it seems like the results come back faster than we can output them.

LLaMA Usability Improvements
[x] Add "instructions" and make "content" an array of strings, not a single string.
    This greatly simplifies telling LLaMA that there is content that cannot be edited down for generation (the
    instructions) and tells it how to edit down the content for generation (the content).
[x] Add "llm settings" subcommand to list settings visible to the llm command.
    This can at least help show what could be set in the environment to control the llm command.
[x] Add functional predict and model options to improve llama usability.
    It's still really hard to understand all the options that llama supports, especially with the abstractions in 
    play.
[x] Adjust the generic interface to accept prediction configuration.
[x] Drop complicated configuration interface for a map[string]string.
    The configuration interface is a bit too complicated for what we need.  A simple map[string]string will do.

Bug Fixes for LLaMA
[x] If len(tokens) == 0, llama.Model.next will panic.  (Inherited from ollama)
[x] Ensure stop words actually stop producing words.
    The stop words implementation has issues where it releases text to output that is part of a stop word, therefore
    missing a stop word.
[x] Rework llama, stripping out weird configuration state and exposing lower level eval / sample methods.
[x] Ensure worker passes predict options on to the client.
    Right now we have to set llm_stop in the worker environment along with other crap.
[x] Why does llama.Model.Predict call the callback once with an empty output at the end of generation?
    Does not happen now that it has been reworked.
[x] Dump superfluous configuration nonsense now that we have configuration structures and functional options.
[ ] Restore timing of eval, etc.
[ ] Restore interruption and reuse of llm interface.
[ ] Document or reimplement ggml-metal.metal hack
    Ollama embeds this file, writing it to filepath.Dir(os.Args[0]), which is bound to upset read-only bin directories
    like those found in Nix.

Restore Embedding Support
[ ] Ollama.ai's implementation supported fetching the embedding and providing it again.
    This is useful for applications that want to reuse embeddings to jumpstart predictions or use vector databases
    to store and find similar embeddings for context.

Support Language Models Over HTTP
[ ] Implement "llm server", an HTTP service that can serve pluggable LLM models.
[ ] Implement "llama-server" mimic API in "llm server"
[ ] Implement "Kobold AI" mimic API in "llm server"
[ ] Implement "OpenAI" mimic API in "llm server"
[ ] Add Autocert support.
    Nobody wants their predictions running around the internet nekkid.
[ ] Add support for API keys.
    Give the server a list of API key hashes and add a "llm apikey" command to generate them.
[ ] Add NGrok support.
    Exposing a language model over NGrok will simplify using third party AI frontends, but we need to have API keys
    first.

Catch up on GGUF Changes
[ ] Catch up to llama.cpp after their beta changes by only supporting loading GGUF for Llama. (We are currently on 
    dadbed99e65252d79f81101a392d0d6497b86caa)
[ ] Add support for GGUF to Llama
[ ] Report model options leveraging GGUF

Support Better Output Constraints
[ ] Support for terminating predictions from Go.
    The prediction type should have a method that can be used to terminate prediction.  Ideally, we also want to
    rewind the prediction, since it is not always clear what is being predicted from a single token -- this is
    a fundamental problem for "token healing".
[ ] Expose the EOL and EOS tokens from the llama Interface
    When writing a prediction handler which can terminate a prediction, knowing when an "EOL" is predicted can be
    a useful signal to stop.  Simply adding EOL as a stop token does not work well.
[ ] GBNF grammars provide a different way to constrain predicted output to a required format.
[ ] Implement support for writing output constraints in Go.

Explore Adding RWKV
[ ] Add RWKV as an LLM implementation alternate to Llama.  This uses GGML and is really pretty similar to llama.cpp,
    but having a second implemenation might keep the llm package from being the llama package in a fancy coat.

Technical Documentation
[ ] Document NATS setup.
    A bootstrap task for llm might make life easier, setting up a TLS CA and generating certificates for the NATS
    workers and clients.  This might be even nicer if we embed the NATS server.
[ ] Document log/slog usage for clients to debug.
[ ] Tutorial on NATS setup.
